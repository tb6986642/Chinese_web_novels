{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1aa118",
   "metadata": {},
   "source": [
    "# Scraping specific information from detail pages using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc7a74",
   "metadata": {},
   "source": [
    "The objective of this section is to scrape specific information from the detail pages of all Chinese novels on the Wuxia World website. The key information includes the title, author, genres, rating, number of chapters, number of reviews, the titles of all the chapters, and details of the reviews.\n",
    "\n",
    "The essence and basic process of web scraping, please refer to https://rayobyte.com/blog/web-crawling-vs-web-scraping/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226e0c8",
   "metadata": {},
   "source": [
    "## 1.Installing Selenium and Browser Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be238c19",
   "metadata": {},
   "source": [
    "In this case, we chose to use Selenium because the Wuxia World website has implemented an anti-scraping mechanism that requires browser simulation to obtain complete data. For installing and documentation, please refer to https://selenium-python.readthedocs.io/index.html\n",
    "\n",
    "Note: Please make sure to install the necessary libraries such as Selenium and BeautifulSoup before running the code. You can use pip, the Python package installer, to install these libraries. To install them, open your command prompt or terminal and enter the following commands:\n",
    "\n",
    "pip install selenium;\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "69ad2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from lxml import etree\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8220ac",
   "metadata": {},
   "source": [
    "Here, we take ChromeDriver as an example. Please follow the setup instruction https://sites.google.com/chromium.org/driver/getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed9205",
   "metadata": {},
   "source": [
    "## 2. Analyzing the webpage structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f91a2d",
   "metadata": {},
   "source": [
    "Firstly, we need to browse the page structure of the target website and examine the web pages to determine their types.\n",
    "\n",
    "Based on observation, we have noticed that all the Chinese novels are listed in alphabetical order in a table on the webpage. Each target novel's title corresponds to a hyperlink that leads to its detail page. In the table, only partial information, such as the novel's name and rating, is displayed. To access more detailed information, one needs to click on the hyperlink. Therefore, our overall web scraping approach is as follows:\n",
    "\n",
    "(1).Obtain a complete list of links corresponding to all the novels;\n",
    "(2).Iterate through the target link list to scrape specific information from each webpage and store it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a17f6",
   "metadata": {},
   "source": [
    "## 3. Obtaining the URL list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d480d5",
   "metadata": {},
   "source": [
    "Using the developer tool, we discovered that the URL is contained within the \"href\" attribute of the novel's title. To obtain a complete list of URLs for the novels, we will follow the steps of locating the novel's title and extracting the \"href\" attribute value of the element.\n",
    "\n",
    "Selenium provides various methods for locating elements, such as XPath, ID, and name. In this case, we will use XPath to locate the element. Once we have obtained the XPath of the corresponding element, we will use the syntax \"@href\" to select the element's attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df16997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-90-c656d770cb62>:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver')\n"
     ]
    }
   ],
   "source": [
    "# Specify the path of the chromedriver and launch the browser\n",
    "driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver')\n",
    "# Navigate to the URL of wuxiaworld\n",
    "driver.get('https://www.wuxiaworld.com/novels')\n",
    "\n",
    "# Wait for 10 seconds for the page to load\n",
    "time.sleep(10)\n",
    "    \n",
    "# Click the 'Chinese' checkbox to show only Chinese novels\n",
    "driver.find_element(By.XPATH,'//*[@id=\"loading-container-replacement\"]/div/div[1]/div[2]/div/div/div/div[1]/div/label[2]/span[2]').click()\n",
    "\n",
    "# Wait for 10 seconds for the page to load\n",
    "time.sleep(10)\n",
    "    \n",
    "# Scroll down the page multiple times to load all the novel data\n",
    "for i in range(60):\n",
    "    # Simulate the Page Down key press using ActionChains\n",
    "    ActionChains(driver).key_down(Keys.PAGE_DOWN).key_up(Keys.PAGE_DOWN).perform()\n",
    "    # Wait for a short time for the page to load\n",
    "    time.sleep(0.2)\n",
    "        \n",
    "# Create an empty list to store all Chinese novels URLs\n",
    "all_url_list = []\n",
    "\n",
    "# Xpath locate the element\n",
    "url_xpath = '//*[@id=\"loading-container-replacement\"]/div/div[2]/div/div/div/div/div/div/div/div/div[2]/p/a/@href'\n",
    "\n",
    "# Get the HTML source of the page\n",
    "html = driver.page_source\n",
    "# Parse the HTML source using the etree.HTML function\n",
    "tree = etree.HTML(html)\n",
    "# Extract the list of URLs using the XPath expression\n",
    "url_list = tree.xpath(url_xpath)\n",
    "# Extend the all_url_list with the extracted URL list\n",
    "all_url_list.extend(url_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a090e",
   "metadata": {},
   "source": [
    "## 4. Iterating through the URL list to scrape specific information from the detail pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911dcb2c",
   "metadata": {},
   "source": [
    "After obtaining the complete URL list of the novels, we need to iterate through the list to retrieve the details page of each novel. On this page, the key content we need to scrape includes:\n",
    "\n",
    "(1)Title;\n",
    "(2)Author;\n",
    "(3)Genres;\n",
    "(4)Rating;\n",
    "(5)Number of chapters;\n",
    "(6)Number of reviews;\n",
    "(7)The titles of all the chapters;\n",
    "(8)Details of the reviews.\n",
    "\n",
    "Please note that the above content is the crucial information we need to extract from the details page of each novel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5738edf",
   "metadata": {},
   "source": [
    "### 4.1 Scraping the Title, Author, Genres, Rating, Number of chapters, and Number of reviews from the novel pages corresponding to each URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe5dd4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-54f9fccd8d19>:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver', options=CHROME_OPTIONS)\n"
     ]
    }
   ],
   "source": [
    "# Browser driver configuration and usage\n",
    "# Create an instance of ChromeOptions to customize Chrome browser settings\n",
    "CHROME_OPTIONS = webdriver.ChromeOptions()\n",
    "# Specify the preference for displaying images\n",
    "# 1 for displaying images, 2 for not displaying images. When images are not needed to be crawled, they can be set to not load images to save time.\n",
    "prefs = {\"profile.managed_default_content_settings.images\":2}   \n",
    "# Add the image display preference to the Chrome options\n",
    "CHROME_OPTIONS.add_experimental_option(\"prefs\", prefs)\n",
    "# Create a Chrome WebDriver instance with the specified driver executable path and options\n",
    "driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver', options=CHROME_OPTIONS)\n",
    "\n",
    "# Xpath locating for basic novel information\n",
    "title_xpath = '//*[@id=\"loading-container-replacement\"]/div/div[1]/div/div/div[2]/div[1]/div[2]/h1'\n",
    "author_xpath = '//*[@id=\"loading-container-replacement\"]/div/div[1]/div/div/div[2]/div[3]/div[1]/div[2]'\n",
    "genres_xpath = '//*[@id=\"full-width-tabpanel-0\"]/div/div[1]/div[2]/div/a/div/div'\n",
    "rating_xpath = '//*[@id=\"loading-container-replacement\"]/div/div[1]/div/div/div[2]/div[2]/div/span/span'\n",
    "chapters_xpath = '//*[@id=\"full-width-tabpanel-0\"]/div/div[1]/div[1]/div[1]/div[2]'\n",
    "reviews_xpath = '//*[@id=\"loading-container-replacement\"]/div/div[1]/div/div/div[2]/div[2]/div/div/span'\n",
    "\n",
    "# Create an empty list to store all the titleï¼Œauthor,genres,rating,chapters and reviews \n",
    "all_title = []\n",
    "all_author = []\n",
    "all_genres = []\n",
    "all_rating = []\n",
    "all_chapters = []\n",
    "all_reviews = []\n",
    "\n",
    "# Iterate through the list of URLs\n",
    "for url_item in all_url_list:\n",
    "    url = 'https://www.wuxiaworld.com' + url_item\n",
    "    driver.get(url) \n",
    "    time.sleep(5)\n",
    "    # Parse the webpage\n",
    "    html = driver.page_source\n",
    "    tree = etree.HTML(html)\n",
    "    \n",
    "    # Extract the text nodes of novel basic information\n",
    "    title_list = tree.xpath(title_xpath + '/text()')\n",
    "    all_title.append(title_list[0])\n",
    "    \n",
    "    author_list = tree.xpath(author_xpath + '/text()')\n",
    "    all_author.append(author_list[0])\n",
    "    \n",
    "    genres_list = tree.xpath(genres_xpath+ '/text()')\n",
    "    content = ';'.join(genres_list)  # There may be multiple genres, concatenate them with a semicolon\n",
    "    all_genres.append(content)\n",
    "    \n",
    "    rating_list = tree.xpath(rating_xpath + '/text()')\n",
    "    all_rating.append(rating_list[0])\n",
    "    \n",
    "    chapters_list = tree.xpath(chapters_xpath + '/text()')\n",
    "    all_chapters.append(chapters_list[0])\n",
    "    \n",
    "    reviews_list = tree.xpath(reviews_xpath + '/text()')\n",
    "    all_reviews.append(reviews_list[0])\n",
    "\n",
    "# Specify the file path and open the CSV file in write mode with UTF-8 encoding    \n",
    "with open('data_list.csv', 'w',encoding='utf-8',newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header row with column names\n",
    "    writer.writerow(['title','author','genres','rating','chapters','reviews','url'])\n",
    "    # Iterate through each URL and corresponding data, and write them as rows in the CSV file\n",
    "    for url_item, row in zip(all_url_list, zip(all_title, all_author, all_genres, all_rating, all_chapters, all_reviews)):\n",
    "        # Convert the row elements to a list and append the URL item to the end\n",
    "        writer.writerow(list(row) + [f'https://www.wuxiaworld.com{url_item}'])\n",
    "\n",
    "# Close the browser driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc103d6",
   "metadata": {},
   "source": [
    "### 4.2 Scraping the titles of all the chapters from the novel pages corresponding to each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76c4f654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-88-6f54b6ee9d7b>:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver', options=CHROME_OPTIONS)\n"
     ]
    }
   ],
   "source": [
    "# Browser driver configuration and usage\n",
    "# Create an instance of ChromeOptions to customize Chrome browser settings\n",
    "CHROME_OPTIONS = webdriver.ChromeOptions()\n",
    "# Specify the preference for displaying images\n",
    "# 1 for displaying images, 2 for not displaying images. When images are not needed to be crawled, they can be set to not load images to save time.\n",
    "prefs = {\"profile.managed_default_content_settings.images\":2}   \n",
    "# Add the image display preference to the Chrome options\n",
    "CHROME_OPTIONS.add_experimental_option(\"prefs\", prefs)\n",
    "# Create a Chrome WebDriver instance with the specified driver executable path and options\n",
    "driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver', options=CHROME_OPTIONS)\n",
    "\n",
    "# Open the novel page\n",
    "driver.get('https://www.wuxiaworld.com/novel/tranxending-vision')\n",
    "\n",
    "# Wait for 10 seconds for the page to load\n",
    "time.sleep(10)\n",
    "\n",
    "# Click the 'Chapters' button\n",
    "driver.find_element(By.XPATH,'//*[@id=\"full-width-tab-1\"]').click()\n",
    "\n",
    "# Wait for 10 seconds for the page to load\n",
    "time.sleep(10)\n",
    "\n",
    "# Xpath locating for chapter names\n",
    "title_chapters_xpath = '//*[@id=\"full-width-tabpanel-1\"]/div/div[2]/div/div[2]/div/div/div/div/div/div/div/a/div/div[1]/div[1]/span'\n",
    "\n",
    "# Find all elements matching the specified XPath on the webpage\n",
    "chapter_elements = driver.find_elements(By.XPATH, '//*[@id=\"full-width-tabpanel-1\"]/div/div[2]/div/div[1]/div[1]/section/div/span')\n",
    "# Determine the number of chapters by counting the elements found\n",
    "num_chapters = len(chapter_elements)\n",
    "\n",
    "# Create an empty list to store all the title chapters\n",
    "all_title_chapters=[]\n",
    "\n",
    "# Iterate through each fascicle\n",
    "for i in range(num_chapters):\n",
    "    # Click on each fascicle\n",
    "    chapter_elements[i].click()\n",
    "    time.sleep(5)\n",
    "    # Parse the webpage\n",
    "    html = driver.page_source\n",
    "    tree = etree.HTML(html)\n",
    "    # Extract the text nodes of chapter names using the specified XPath\n",
    "    title_chapters = tree.xpath(title_chapters_xpath + '/text()')\n",
    "    # Join the extracted chapter names into a single string separated by newline characters\n",
    "    content = '\\n'.join(title_chapters)\n",
    "    # Append the content (chapter names) to the list of all_title_chapters\n",
    "    all_title_chapters.append(content)\n",
    "\n",
    "# Specify the file path where the chapter names will be written\n",
    "file_path = os.path.join(os.getcwd(), 'all_title_chapters.txt')\n",
    "# Open the file in write mode and specify the encoding as UTF-8\n",
    "with open(file_path, 'w', encoding='utf-8') as f:\n",
    "    # Write the content of the last loop (i.e., all chapter names) to the file\n",
    "    f.write(all_title_chapters[-1])\n",
    "\n",
    "# Close the browser driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c79c04",
   "metadata": {},
   "source": [
    "### 4.3 Scraping the details of the reviews from the novel pages corresponding to each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "465226b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-92-6a0aa88beaf5>:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver')\n"
     ]
    }
   ],
   "source": [
    "# Specify the path of the chromedriver and launch the browser\n",
    "driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver')\n",
    "# Open the novel page\n",
    "driver.get('https://www.wuxiaworld.com/novel/rmji')\n",
    "\n",
    "# Wait for 10 seconds for the page to load\n",
    "time.sleep(10)\n",
    "\n",
    "# Replace the generic view_all path\n",
    "driver.find_element(By.XPATH,'//*[@id=\"full-width-tabpanel-0\"]/div/div[3]/div[2]/div[2]/div/div[2]/div/span').click()\n",
    "\n",
    "# Wait for 10 seconds for the page to load\n",
    "time.sleep(10)\n",
    "\n",
    "# Create an empty list to store all the detail reviews\n",
    "detail_reviews_list= []\n",
    "\n",
    "# Get the reviews on the first page\n",
    "html = driver.page_source # Get the HTML source code of the current page\n",
    "soup = BeautifulSoup(html, \"html.parser\") # Create a BeautifulSoup object to parse the HTML\n",
    "data = soup.find_all('div', class_=\"absolute top-0 -z-10 line-clamp-1 font-set-r15-h150 text-gray-t1 sm2:font-set-r16-h150\")\n",
    "# Find all the review elements with the specified class\n",
    "# Note: The class represents the CSS styles applied to the review elements\n",
    "\n",
    "data = data[3:] # Remove the first three reviews from the list of review elements\n",
    "detail_reviews_list.extend(data) # Add the remaining reviews to the detail_reviews_list\n",
    "\n",
    "# Use a set to keep track of already retrieved reviews\n",
    "seen_reviews = set([r.get_text(strip=True) for r in detail_reviews_list])\n",
    "\n",
    "# Get the reviews from the next page\n",
    "next_page=driver.find_element(By.XPATH,'/html/body/div[2]/div[3]/div/div/div/div[2]/div[3]/nav/ul/li[last()-0]/button')\n",
    "\n",
    "# Keep clicking the next page button until it's disabled\n",
    "while next_page.is_enabled():\n",
    "    next_page.click()\n",
    "    time.sleep(5)\n",
    "    html = driver.page_source # Get the HTML source code of the current page\n",
    "    soup = BeautifulSoup(html, \"html.parser\") # Create a BeautifulSoup object to parse the HTML\n",
    "    data = soup.find_all('div', class_=\"absolute top-0 -z-10 line-clamp-1 font-set-r15-h150 text-gray-t1 sm2:font-set-r16-h150\")\n",
    "    # Find all the review elements with the specified class\n",
    "    # Note: The class represents the CSS styles applied to the review elements\n",
    "    \n",
    "    # Add the unseen reviews to the detail_reviews_list\n",
    "    for review in data:\n",
    "        if review.get_text(strip=True) not in seen_reviews:\n",
    "            detail_reviews_list.append(review)\n",
    "            seen_reviews.add(review.get_text(strip=True))\n",
    "    # Find the next page button and wait until it is present\n",
    "    next_page = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH,'/html/body/div[2]/div[3]/div/div/div/div[2]/div[3]/nav/ul/li[last()-0]/button')))\n",
    "\n",
    "# Open a text file named 'detail_reviews.txt' in write mode, using UTF-8 encoding\n",
    "with open('detail_reviews.txt', 'w', encoding='utf-8') as f:\n",
    "    # Iterate through each detail_review in the detail_reviews_list\n",
    "    for detail_review in detail_reviews_list:\n",
    "        # Write the stripped text of the detail_review to the file\n",
    "        f.write(detail_review.get_text(strip=True))\n",
    "        # Write two newline characters to create a blank line between reviews\n",
    "        f.write('\\n\\n')\n",
    "\n",
    "# Close the browser driver        \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44344e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
